# 9장 - 웹 크롤러 설계

- 로봇, 또는 스파이더라고 부름
- 웹에 새로 올라오거나 갱신된 콘텐츠를 찾아오는 것이 주된 목적
  - 웹 페이지, 이미지, 비디오, PDF ..
- 몇 개 웹 페이지의 링크를 따라 나가면서 새로운 콘텐츠 수집
- 크롤러를 이용하는 곳
  - 검색 엔진 인덱스 : 구글 검색 엔진이 사용, 가장 보편적인 용례
  - 웹 아카이빙 : 장기 보관을 위해 웹에서 정보를 모음
  - 웹 마이닝 : 인터넷에서 유용한 지식을 도출
  - 웹 모니터링 : 저작권이나 상표권이 침해되는 상황을 모니터링
- 처리해야 하는 데이터의 규모에 따라 웹 크롤러의 복잡도가 달라짐

## 1단계 : 문제 이해 및 설계 범위 확정

- 웹 크롤러의 기본 알고리즘
  - URL 집합이 입력으로 주어지면 해당 URL이 가리키는 모든 웹 페이지를 다운로드
  - 다운로드한 웹 페이지에서 URL들을 추출
  - 추출된 URL들을 다운로드할 URL 목록에 추가하고 위의 과정을 처음부터 반복
- 설계를 진행하기 전 알아내야 할 요구사항
  - 크롤러의 주된 용도
  - 매달 수집해야 되는 웹 페이지 개수
  - 새로 만들어지거나 수정된 웹 페이지도 고려하는지?
  - 수집한 웹페이지는 저장해야 하는지?
  - 중복된 콘텐츠의 처리
- 웹 크롤러가 만족시켜야 할 속성
  - 규모 확장성 (병행성을 활용할 것)
  - 안정성 (장애, 악성코드 등의 함정 주의)
  - 예절 (수집 대상 웹 사이트에 많은 요청을 보내서는 안 됨)
  - 확장성 (새로운 형태의 콘텐츠를 지원하기 쉬워야 함)

### 개략적 규모 추정

- 매달 10억 개의 웹 페이지를 다운
  - QPS = 대략 400 페이지/초
  - 최대 QPS = 2 x QPS = 800
- 웹 페이지의 크기 평균은 500k라고 가정
  - 10억 페이지일 시 월 500TB
  - 5년간 보관 시 30PB의 저장 용량 필요

## 2단계 : 개략적 설계안 제시 동의 및 구하기

### 시작 URL 집합

- 웹 크롤러가 크롤링을 시작하는 출발점
- 크롤러가 가능한 많은 링크를 탐색할 수 있도록 하는 URL을 골라야 함
  - 전체 URL 공간을 작은 방법으로 나누거나 주제별로 다른 시작 URL을 사용

### 미수집 URL 저장소

- `다운로드할 URL`인 크롤링 상태를 저장 관리하는 컴포넌트
- FIFO 큐

### HTML 다운로더

- 인터넷에서 웹 페이지를 다운로드하는 컴포넌트

### 도메인 이름 변환기

- URL에 대응되는 IP 주소를 알아냄

### 콘텐츠 파서

- 웹 페이지 다운 시 파싱과 검증을 해줌

### 중복 콘텐츠인가?

- 자료 구조를 도입하여 데이터 중복을 줄이고 데이터 처리에 소요되는 시간을 줄임
- 웹 페이지의 해시 값을 비교하는 것이 효과적

### 콘텐츠 저장소

- HTML 문서를 보관하는 시스템
- 저장소를 구현하는데 쓰일 기술을 고를 땐 데이터의 유형, 크기, 저장소 접근 빈도, 데이터의 유효 기간 등을 종합적으로 고려
  - 데이터의 양이 너무 많으므로 대부분의 콘텐츠는 디스크에 저장
  - 인기 있는 콘텐츠는 메모리를 두어 접근 지연시간을 줄일 것

### URL 추출기

- HTML 페이지를 파싱 하여 링크들을 골라내는 역할을 함
  - 상대 경로를 전부 절대 경로로 변환 등

### URL 필터

- 특정 URL을 필터링해서 크롤링 대상에서 배제
  - 특정한 콘텐츠 타입이나 파일 확장자를 갖는 URL, 접속 시 오류가 발생하는 URL, 접근 제외 목록에 포함된 URL 등

### 이미 방문한 URL?

- 이미 방문한 URL이나 미수집 URL 저장소에 보관된 URL을 추적할 수 있도록 하는 자료 구조 사용
  - URL을 여러 번 추적하는 일을 방지할 수 있음 (서버 부하를 줄이고 무한 루프 방지)
- 블룸 필터나 해시 테이블이 널리 쓰임

### URL 저장소

- 이미 방문한 URL을 보관하는 저장소

### 웹 크롤러 작업 흐름

- 시작 URL들을 미수집 ULR 저장소에 저장
- HTML 다운로더는 미수집 URL 저장소에서 URL 목록을 가져옴
- 도메인 이름 변환기를 사용하여 IP 주소를 알아낸 후 접속하여 웹 페이지 다운
- 콘텐츠 파서가 다운된 HTML 페이지 파싱 후 검증
- 중복 콘텐츠 인지 확인
  - 이미 저장소에 있는 경우 처리하지 않고 버림
  - 저장소에 없는 콘텐츠는 저장소에 저장한 뒤 URL 추출기로 전달
- URL 추출기는 HTML 페이지에서 링크를 골라냄
- 골라낸 링크를 URL 필터로 전달
- 필터링이 끝나고 남은 URL만 중복 URL 판별 단계로 전달
- URL 저장소에 보관된 URL 인지 확인, 이미 저장소에 있는 URL은 버림
- 저장소에 없는 URL은 저장소와 미수집 URL 저장소에도 전달

## 3단계 : 상세 설계

DFS를 쓸 것인가, BFS를 쓸 것인가

- 웹은 유향 그래프
  - 페이지는 노드, 하이퍼링크는 에지
- DFS(깊이 우선 탐색법)은 좋은 선택이 아닐 가능성이 높음
  - 그래프 크기가 클 경우 어느 정도로 깊숙이 가게 될지 가늠이 어려움
- 웹 크롤러는 보통 BFS(너비 우선 탐색법)을 사용
  - FIFO 큐를 사용하는 알고리즘
  - 크롤러가 같은 호스트에 속한 많은 링크를 다운로드하느라 서버 과부하에 걸리게 할 수 있는 문제 (예의 없는 크롤러)
  - 표준적 BFS 알고리즘은 URL 간에 우선순위를 두지 않는 문제

미수집 URL 저장소

- 미수집 URL 저장소를 활용하여 예의를 갖춘 크롤러를 만들고 URL 사이의 우선순위와 신선도를 구별하는 크롤러 구현 가능

예의

- 너무 많은 요청을 보내는 것은 `무례한(impolite)` 일, DoS 공격으로도 간주
- 동일한 웹 사이트에 대해선 한 번에 한 페이지만 요청하는 것이 원칙
- 웹사이트의 호스트명과 다운로드 작업을 수행하는 작업 스레드 사이의 관계를 유지
  - 각 다운로드 스레드가 별도 FIFO 큐를 갖고 해당 큐에서 꺼낸 URL만 다운로드
  - 큐 라우터(같은 호스트는 언제나 같은 큐로), 매핑 테이블(호스트 이름과 큐 사이의 관계를 보관), FIFO 큐
  - 큐 선택기 : 큐 선택기 들은 큐들을 순회하면서 큐에서 URL을 꺼내낸 후 작업 스레드에 전달
  - 작업 스레드 : 전달된 URL을 다운로드하는 작업, 작업들 사이에 지연시간을 둘 수 있음

우선순위

- 크롤러 입장에서 중요한 페이지를 먼저 수집하도록 하는 것이 바람직
- URL 우선순위는 페이지랭크, 트래픽 양, 갱신 빈도 등 다양한 척도 사용
- URL 우선순위를 고려한 설계
  - 순위 결정 장치가 URL을 입력받아 우선순위 계산
  - 우선순위별로 큐가 하나씩 할당
  - 큐 선택기 : 임의 큐에서 처리할 URL을 꺼내는 역할을 담당
  - 전면 큐 : 우선순위 결정 과정을 처리
  - 후면 큐 : 크롤러가 예의 바르게 동작하도록 보증

신선도

- 데이터의 신선함을 유지하기 위해 이미 다운로드한 페이지라도 주기적으로 재수집 해야 함
- 재수집 최적화 전략
  - 웹 페이지의 변경 이력 활용
  - 우선순위를 활용하여 중요한 페이지는 좀 더 자주 재수집

미수집 URL 저장소를 위한 지속성 저장 장치

- URL 모두 메모리에 보관하는 것이나 모두 디스크에 저장하는 것은 좋은 방법이 아니므로 절충안이 필요
  - 대부분 URL은 디스크에 두지만 IO 비용을 줄이기 위해 메모리 버퍼에 큐를 둠
  - 버퍼에 있는 데이터는 주기적으로 디스크에 기록

### HTML 다운로더

- HTML 다운로더는 HTTP 프로토콜을 통해 웹 페이지를 내려받음

Robots.txt

- 웹 사이트가 크롤러와 소통하는 표준적 방법
- 크롤러가 수집해도 되는 페이지 목록이 들어 있음
- 주기적으로 다운로드해 캐시에 보관

성능 최적화

- 분산 크롤링 : 크롤링 작업을 여러 서버에 분산
- 도메인 이름 변환 결과 캐시 : DNS 조회 결과로 얻어진 도메인 이름과 IP 주소 사이의 관계를 캐시에 보관 후 주기적으로 갱신
- 지역성 : 크롤링 작업을 수행하는 서버를 지역별로 분산
- 짧은 타임아웃 : 최대 얼마나 기다릴지를 미리 정해두는 것

안정성

- 안정 해시 : 다운로더 서버들에 부하를 분산할 때 적용
- 크롤링 상태 및 수집 데이터 저장 : 장애가 발생한 경우에도 쉽게 복구할 수 있도록 기록
- 예외 처리
- 데이터 검증

확장성

- 새로운 형태의 콘텐츠를 쉽게 지원할 수 있도록 신경 써야 함
  - 새로운 모듈을 끼워 넣음으로써 새로운 형태의 콘텐츠를 지원할 수 있도록 설계

문제 있는 콘텐츠 감지 및 회피

- 중복 콘텐츠
  - 해시, 체크섬을 사용해 탐지
- 거미 덫
  - 크롤러를 무한 루프에 빠뜨리도록 설계한 웹 페이지
  - URL 최대 길이를 제한하므로 회피
  - 수작업으로 덫을 확인하고 찾아낸 후에 덫이 있는 사이트를 크롤러 탐색 대상에서 제외
- 데이터 노이즈
  - 광고, 스크립트, 스팸 URL 같은 가치 없는 콘텐츠는 제외할 것

## 4단계 : 마무리

- 크롤러가 갖추어야 하는 특성
  - 규모 확장성, 예의, 확장성, 안정성
- 추가적인 주제
  - 서버 측 렌더링
  - 원치 않는 페이지 필터링
  - 데이터베이스 다중화 및 샤딩
  - 수평적 규모 확장성
  - 가용성, 일관성, 안정성
  - 데이터 분석 솔루션
